{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40273f96",
   "metadata": {},
   "source": [
    "# 0. IMPORT LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56899ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dateutil import parser\n",
    "from typing import Optional, List, Tuple\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from transformers import  AutoTokenizer\n",
    "from pypdf import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import kagglehub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c3a68",
   "metadata": {},
   "source": [
    "# 1. IMPORT LIB & DATA SOUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742de5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huy.let3\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\huy.let3\\.cache\\kagglehub\\datasets\\snehaanbhawal\\resume-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"snehaanbhawal/resume-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e1031",
   "metadata": {},
   "source": [
    "# 2. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "036f6566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for 2484 resumes\n",
      "Loading resumes from category: INFORMATION-TECHNOLOGY\n",
      "  Loaded 247 resumes from INFORMATION-TECHNOLOGY\n",
      "Total resumes loaded: 247\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = r\"C:\\Users\\huy.let3\\Desktop\\resume\\data\\data\"\n",
    "\n",
    "CSV_PATH = r\"C:\\Users\\huy.let3\\Desktop\\resume\\Resume\\Resume.csv\"  # Path to the CSV file with resume metadata\n",
    "\n",
    "resume_df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded metadata for {len(resume_df)} resumes\")\n",
    "\n",
    "resume_df = resume_df.loc[resume_df['Category'] == 'INFORMATION-TECHNOLOGY']\n",
    "resume_df\n",
    "\n",
    "# List to store all loaded documents\n",
    "all_documents = []\n",
    "\n",
    "# Check if the data directory exists\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(f\"Warning: Data directory '{DATA_DIR}' not found. Please check the path.\")\n",
    "else:\n",
    "    # Get all categories (subdirectories)\n",
    "    categories = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
    "    \n",
    "    for category in categories:\n",
    "        category_path = os.path.join(DATA_DIR, category)\n",
    "        print(f\"Loading resumes from category: {category}\")\n",
    "        \n",
    "        # Use DirectoryLoader to load all PDFs in the category directory\n",
    "        loader = DirectoryLoader(\n",
    "            category_path, \n",
    "            glob=\"**/*.pdf\",  # Load all PDFs, including in subdirectories\n",
    "            loader_cls=PyPDFLoader\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            docs = loader.load()\n",
    "            # Add metadata: include the category and filename\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"category\"] = category\n",
    "                filename = os.path.basename(doc.metadata[\"source\"])\n",
    "                doc.metadata[\"file_name\"] = filename\n",
    "                doc.metadata[\"id\"] = os.path.splitext(filename)[0]  # Remove extension to get ID\n",
    "                \n",
    "                # Add additional metadata from CSV if available\n",
    "                if resume_df is not None:\n",
    "                    resume_id = doc.metadata[\"id\"]\n",
    "                    resume_info = resume_df[resume_df[\"ID\"] == resume_id]\n",
    "                    if not resume_info.empty:\n",
    "                        # Add any additional metadata from the CSV\n",
    "                        pass\n",
    "            \n",
    "            all_documents.extend(docs)\n",
    "            print(f\"  Loaded {len(docs)} resumes from {category}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading documents from {category}: {e}\")\n",
    "\n",
    "print(f\"Total resumes loaded: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53503814",
   "metadata": {},
   "source": [
    "# 3. FROM UNSTRUCTURE TO STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_scanned_pdf(pdf_path):\n",
    "    \"\"\"Check if the PDF is scanned by verifying if text can be extracted.\"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            if page.extract_text():\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def extract_text_from_scanned_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a digitally generated (text-based) PDF.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdfPath):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdfPath)\n",
    "        for page in reader.pages:\n",
    "            pageText = page.extract_text()\n",
    "            if pageText:\n",
    "                text += pageText + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdfPath} using PyPDF2: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(\"\\n\", \" \").replace(\"  \", \" \").strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072664bd",
   "metadata": {},
   "source": [
    "# 4. LLM MODEL (OPENAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb24364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = \"your_key\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4\",  # or \"gpt-3.5-turbo\"\n",
    "    temperature=0.2,\n",
    "    openai_api_key=\"YOUR KEY\"  # Or set via ENV variable: OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "def extract_entities_with_gpt(text):\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant that extracts exact structured candidate information from resumes.\n",
    "\n",
    "    Please extract exactly words the following fields and return the result in valid JSON format:\n",
    "\n",
    "    - Name: full name of the candidate if not label NA\n",
    "    - Email: valid email address need contain @ in results;if not label NA\n",
    "    - Phone: phone number ;if not label NA\n",
    "    - Skills: a list of technical skills in IT fields and professional/soft skills in IT Fields\n",
    "    - Education: list of degrees with institution name and graduation year  \n",
    "    - Experience: for each job, include:\n",
    "        - Job Title\n",
    "        - Company Name\n",
    "        - Years Worked: Each jobs calculate by using End Year Minus Start Year and Round up with 1 decimal\n",
    "        - Short Description of Responsibilities: extract exact words\n",
    "    - Certifications: list of relevant certifications (if available)  ; if not label NA\n",
    "    - Languages: languages the candidate can speak or write such as English or Spain or Vietnam; if not label NA \n",
    "\n",
    "    Text:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Respond in JSON format.\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4\",\n",
    "        input=[\n",
    "        {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    if is_scanned_pdf(pdf_path):\n",
    "        print(\"Detected: Scanned PDF (image-based)\")\n",
    "        raw_text = extract_text_from_scanned_pdf(pdf_path)\n",
    "    else:\n",
    "        print(\"Detected: Text-based PDF (digitally generated)\")\n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    cleaned = clean_text(raw_text)\n",
    "    print(\"Sending to GPT for entity extraction...\")\n",
    "    extracted_entities = extract_entities_with_gpt(cleaned)\n",
    "    return extracted_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "500b0dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: Text-based PDF (digitally generated)\n",
      "Sending to GPT for entity extraction...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Name': 'NA',\n",
       " 'Email': 'NA',\n",
       " 'Phone': 'NA',\n",
       " 'Skills': ['Army',\n",
       "  'Cisco',\n",
       "  'counseling',\n",
       "  'customer assistance',\n",
       "  'database',\n",
       "  'documentation',\n",
       "  'Information Technology',\n",
       "  'inventory',\n",
       "  'IP',\n",
       "  'LAN',\n",
       "  'Windows 7',\n",
       "  'Network',\n",
       "  'policies',\n",
       "  'protocols',\n",
       "  'repairs',\n",
       "  'Routing',\n",
       "  'supervisor',\n",
       "  'test equipment',\n",
       "  'troubleshoot',\n",
       "  'WAN'],\n",
       " 'Education': [{'Degree': 'Certification (Windows 7)',\n",
       "   'Institution': 'Microsoft, Fort Bragg, NC',\n",
       "   'Year': '2012'},\n",
       "  {'Degree': 'Certification (Security)',\n",
       "   'Institution': 'Comptia, Yong San, Korea',\n",
       "   'Year': '2012'},\n",
       "  {'Degree': 'Distinguished Graduate Certificate, Information Technology (Network Communications) Course',\n",
       "   'Institution': 'U.S. Army',\n",
       "   'Year': '2009'},\n",
       "  {'Degree': 'Certificate, IT Network and Cisco Routing',\n",
       "   'Institution': 'IT Field Services Branch',\n",
       "   'Year': '2009'},\n",
       "  {'Degree': 'Associate of Science : Radiography',\n",
       "   'Institution': 'Northwest Florida State College',\n",
       "   'Year': '2008'},\n",
       "  {'Degree': 'Radiography Certificate',\n",
       "   'Institution': 'IT Tech Prep, Trumbull Career and Technical Center',\n",
       "   'Year': '2001'},\n",
       "  {'Degree': 'Diploma', 'Institution': 'Warren G. Harding', 'Year': '2001'}],\n",
       " 'Experience': [{'Job Title': 'Information Technology Supervisor',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': '3.4',\n",
       "   'Short Description of Responsibilities': 'Supervise up to 10 personnel at one time, delegating tasks, conducting performance evaluations and providing corrective counseling as necessary. Train personnel in the set-up and proper use of IT related equipment while adhering to all policies and procedures. Responsible for the inventory of over $1 million worth of network communications equipment.'},\n",
       "  {'Job Title': 'Information Technology Technician',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': '2.0',\n",
       "   'Short Description of Responsibilities': 'Maintained communications equipment in order to effectively relay confidential and secret information. Utilized electronic test equipment to troubleshoot malfunctioning communications equipment and complete repairs as necessary. Regularly set up and added computer systems to a communication network, installing operation systems, accessing stored programs and utilizing IP addresses. Received training in LAN/WAN protocols.'},\n",
       "  {'Job Title': 'Radiology Technologist',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': '0.4',\n",
       "   'Short Description of Responsibilities': 'Routinely performed radiological examinations in a medical clinic. Competent and experienced in the set-up and adjustment of medical devices or equipment. Regularly provided customer assistance, ensuring all patients received timely and accurate care. Accountable for the accurate documentation via electronic database and file system ensuring all confidentiality was maintained.'},\n",
       "  {'Job Title': 'Command Post Controller',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': '4.0',\n",
       "   'Short Description of Responsibilities': 'Provided command, control, communications, and information support throughout operations during peacetime, emergency, and disaster situations.'}],\n",
       " 'Certifications': ['Windows 7 (Microsoft, Fort Bragg, NC,  2012)',\n",
       "  'Security  (Comptia, Yong San, Korea,  2012)'],\n",
       " 'Languages': 'NA'}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pdf = r\"data\\data\\INFORMATION-TECHNOLOGY\\33241454.pdf\"\n",
    "results_llm = json.loads(process_pdf(input_pdf))\n",
    "results_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681673c",
   "metadata": {},
   "source": [
    "# 5.Grouth Truth Via with REGEX HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_resume_entities(html_string):\n",
    "    soup = BeautifulSoup(html_string, \"html.parser\")\n",
    "    extracted = {\n",
    "        \"Name\": \"NA\",\n",
    "        \"Email\": \"NA\",\n",
    "        \"Phone\": \"NA\",\n",
    "        \"Skills\": [],\n",
    "        \"Education\": [],\n",
    "        \"Experience\": [],\n",
    "        \"Certifications\": [],\n",
    "        \"Languages\": []\n",
    "    }\n",
    "\n",
    "    # 1. Name (using SECTION_NAME)\n",
    "    name_section = soup.find(\"div\", id=re.compile(r\"SECTION_NAME1\"))\n",
    "    if name_section:\n",
    "        name_text = name_section.get_text(separator=\" \", strip=True)\n",
    "        if name_text:\n",
    "            extracted['Name'] = name_text\n",
    "\n",
    "    # 2. Email\n",
    "    email_match = re.search(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", html_string)\n",
    "    if email_match:\n",
    "        extracted['Email'] = email_match.group()\n",
    "\n",
    "    # 3. Phone\n",
    "    phone_match = re.search(r\"\\b(?:\\+?1[\\s\\-]?)?(?:\\(?\\d{3}\\)?[\\s\\-]?)?\\d{3}[\\s\\-]?\\d{4}\\b\", html_string)\n",
    "    if phone_match:\n",
    "        extracted['Phone'] = phone_match.group()\n",
    "\n",
    "    # 4. Skills\n",
    "    skill_sections = soup.find_all(\"div\", id=re.compile(r\"SECTION_SKLL\"))\n",
    "    for section in skill_sections:\n",
    "        for block in section.find_all(\"div\", class_=\"field\"):\n",
    "            text = block.get_text(separator=\" \", strip=True)\n",
    "            if text:\n",
    "                for skill in re.split(r\"[,;•\\-|\\n]+\", text):\n",
    "                    skill_clean = skill.strip()\n",
    "                    if skill_clean:\n",
    "                        extracted[\"Skills\"].append(skill_clean)\n",
    "\n",
    "    # 5. Education\n",
    "    education_section = soup.find(\"div\", id=re.compile(\"SECTION_EDUC\"))\n",
    "    if education_section:\n",
    "        entries = education_section.find_all(\"div\", class_=\"singlecolumn\")\n",
    "        for entry in entries:\n",
    "            degree = entry.find(\"span\", class_=\"degree\")\n",
    "            program = entry.find(\"span\", class_=\"programline\")\n",
    "            year = entry.find(\"span\", class_=\"jobdates\")\n",
    "            institution = entry.find(\"span\", class_=\"companyname\")\n",
    "\n",
    "            edu_item = {\n",
    "                \"Degree\": degree.get_text(strip=True) if degree else \"NA\",\n",
    "                \"Program\": program.get_text(strip=True) if program else \"NA\",\n",
    "                \"Year\": year.get_text(strip=True) if year else \"NA\",\n",
    "                \"Institution\": institution.get_text(strip=True) if institution else \"NA\"\n",
    "            }\n",
    "            extracted[\"Education\"].append(edu_item)\n",
    "\n",
    "    # 6. Experience\n",
    "    experience_section = soup.find(\"div\", id=re.compile(\"SECTION_EXPR\"))\n",
    "    if experience_section:\n",
    "        jobs = experience_section.find_all(\"div\", class_=\"paragraph\")\n",
    "        for job in jobs:\n",
    "            title = job.find(\"span\", class_=\"jobtitle\")\n",
    "            company = job.find(\"span\", class_=\"companyname\")\n",
    "            description = job.find(\"span\", class_=\"jobline\")\n",
    "            \n",
    "            # Extract exactly 2 jobdates (start and end)\n",
    "            dates = job.find_all(\"span\", class_=\"jobdates\", format=True)  # filter only real dates\n",
    "            years_worked = \"NA\"\n",
    "            import math\n",
    "\n",
    "            if len(dates) == 2:\n",
    "                try:\n",
    "                    start_date = parser.parse(dates[0].get_text(strip=True), dayfirst=False)\n",
    "                    end_date = parser.parse(dates[1].get_text(strip=True), dayfirst=False)\n",
    "                    delta_years = (end_date - start_date).days / 365.25\n",
    "                    years_worked = math.ceil(delta_years * 10) / 10\n",
    "                except Exception:\n",
    "                    years_worked = \"NA\"\n",
    "\n",
    "            exp_item = {\n",
    "                \"Job Title\": title.get_text(strip=True) if title else \"NA\",\n",
    "                \"Company Name\": company.get_text(strip=True) if company else \"NA\",\n",
    "                \"Years Worked\": years_worked,\n",
    "                \"Short Description of Responsibilities\": description.get_text(strip=True) if description else \"NA\"\n",
    "            }\n",
    "            extracted.setdefault(\"Experience\", []).append(exp_item)\n",
    "\n",
    "    # 7. Certifications\n",
    "    cert_match = re.findall(r\"(?:certified|certification|certificate)[^<\\n]{0,100}\", html_string, flags=re.IGNORECASE)\n",
    "    if cert_match:\n",
    "        extracted[\"Certifications\"] = list(set(map(str.strip, cert_match)))\n",
    "    # 8. Languages\n",
    "    lang_keywords = re.findall(r\"\\b(English|Spanish|Vietnamese|Chinese|French|German|Japanese|Korean)\\b\", html_string, flags=re.IGNORECASE)\n",
    "    if lang_keywords:\n",
    "        extracted[\"Languages\"] = list(set(lang_keywords))\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e222c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'NA',\n",
       " 'Email': 'NA',\n",
       " 'Phone': 'NA',\n",
       " 'Skills': ['Army',\n",
       "  'Cisco',\n",
       "  'counseling',\n",
       "  'customer assistance',\n",
       "  'database',\n",
       "  'documentation',\n",
       "  'Information Technology',\n",
       "  'inventory',\n",
       "  'IP',\n",
       "  'LAN',\n",
       "  'Windows 7',\n",
       "  'Network',\n",
       "  'personnel',\n",
       "  'policies',\n",
       "  'protocols',\n",
       "  'repairs',\n",
       "  'Routing',\n",
       "  'San',\n",
       "  'supervisor',\n",
       "  'test equipment',\n",
       "  'troubleshoot',\n",
       "  'WAN'],\n",
       " 'Education': [{'Degree': 'Certification, Windows 7, Microsoft, Fort Bragg, NC,',\n",
       "   'Program': '',\n",
       "   'Year': '2012',\n",
       "   'Institution': ''},\n",
       "  {'Degree': '*Certification, Security  , Comptia, Yong San, Korea,',\n",
       "   'Program': '',\n",
       "   'Year': '2012',\n",
       "   'Institution': ''},\n",
       "  {'Degree': '*Distinguished Graduate Certificate, Information Technology (Network Communications) Course',\n",
       "   'Program': '',\n",
       "   'Year': '2009',\n",
       "   'Institution': 'U.S. Army'},\n",
       "  {'Degree': 'Certificate, IT Network and Cisco Routing, IT Field Services Branch',\n",
       "   'Program': '',\n",
       "   'Year': '2009',\n",
       "   'Institution': ''},\n",
       "  {'Degree': 'Associate of Science',\n",
       "   'Program': 'Radiography',\n",
       "   'Year': '2008',\n",
       "   'Institution': 'Northwest Florida State College'},\n",
       "  {'Degree': 'Certificate',\n",
       "   'Program': '',\n",
       "   'Year': '2001',\n",
       "   'Institution': 'IT Tech Prep, Trumbull Career and Technical Center'},\n",
       "  {'Degree': 'Diploma',\n",
       "   'Program': '',\n",
       "   'Year': '2001',\n",
       "   'Institution': 'Warren G. Harding'}],\n",
       " 'Experience': [{'Job Title': 'Information Technology Supervisor',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': 3.4,\n",
       "   'Short Description of Responsibilities': 'Supervise up to 10 personnel at one time, delegating tasks, conducting performance evaluations and providing corrective counseling as necessary.Train personnel in the set-up and proper use of IT related equipment while adhering to all policies and procedures.Responsible for the inventory of over $1 million worth of network communications equipment.Tasked by President of the United States to act as supervisor and maintain signal communications for Fort Bragg army base.'},\n",
       "  {'Job Title': 'Information Technology Technician',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': 2.0,\n",
       "   'Short Description of Responsibilities': 'Maintained communications equipment in order to effectively relay confidential and secret information.Utilized electronic test equipment to troubleshoot malfunctioning communications equipment and complete repairs as necessary.Regularly set up and added computer systems to a communication network, installing operation systems, accessing stored programs and utilizing IP addresses.Received training in LAN/WAN protocols.'},\n",
       "  {'Job Title': 'Radiology Technologist',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': 0.4,\n",
       "   'Short Description of Responsibilities': 'Routinely performed radiological examinations in a medical clinic.Competent and experienced in the set-up and adjustment of medical devices or equipment.Regularly provided customer assistance, ensuring all patients received timely and accurate care.Accountable for the accurate documentation via electronic database and file system ensuring all confidentiality was maintained.'},\n",
       "  {'Job Title': 'Command Post Controller',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': 4.0,\n",
       "   'Short Description of Responsibilities': 'Provided command, control, communications, and information support throughout operations during peacetime, emergency, and disaster situations.Received and relayed instructions and records, submitting manual and automated data products.Disseminated time-sensitive critical information to senior leaders and support agencies.'}],\n",
       " 'Certifications': ['Certification, Windows 7, Microsoft, Fort Bragg, NC,',\n",
       "  'Certificate',\n",
       "  'Certificate of Achievement for outstanding support as a member of the Tiger Team during the Windows 7 Migration',\n",
       "  'Certification and a Secret Security Clearance.',\n",
       "  'Certification, Security  , Comptia, Yong San, Korea,',\n",
       "  'Certificate, IT Network and Cisco Routing, IT Field Services Branch',\n",
       "  'Certificate, Information Technology (Network Communications) Course'],\n",
       " 'Languages': []}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = extract_resume_entities(resume_df.loc[resume_df['ID'] == 33241454]['Resume_html'].values[0])\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f0740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ✅ Match (Similarity: 1.00)\n",
      "Email: ✅ Match (Similarity: 1.00)\n",
      "Phone: ✅ Match (Similarity: 1.00)\n",
      "Skills: ✅ Match (Similarity: 1.00)\n",
      "Education: ✅ Match (Similarity: 0.96)\n",
      "Experience: ✅ Match (Similarity: 0.76)\n",
      "Certifications: ❌ Mismatch (Similarity: 0.60)\n",
      "Languages: ❌ Mismatch (Similarity: 0.26)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load small embedding model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def compute_similarity(text1, text2):\n",
    "    embeddings = model.encode([text1, text2], convert_to_tensor=True)\n",
    "    return float(util.cos_sim(embeddings[0], embeddings[1]))\n",
    "\n",
    "# Set a reasonable threshold for similarity\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "\n",
    "comparison = {}\n",
    "for key in ground_truth:\n",
    "    response_text = str(results_llm.get(key, \"\")).replace(\"\\n\", \" \").strip().lower()\n",
    "    expected_text = str(ground_truth.get(key, \"\")).replace(\"\\n\", \" \").strip().lower()\n",
    "    similarity = compute_similarity(response_text, expected_text)\n",
    "    comparison[key] = similarity >= SIMILARITY_THRESHOLD\n",
    "\n",
    "    print(f\"{key}: {'✅ Match' if comparison[key] else '❌ Mismatch'} (Similarity: {similarity:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e56ac",
   "metadata": {},
   "source": [
    "# 6. SELF-EVALUATION LLM (Context vs Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \", \"```\\n\", \"\\n\\\\*\\\\*\\\\*+\\n\", \"\\n---+\\n\", \"\\n___+\\n\", \"\\n\\n\", \"\\n\", \" \", \"\"\n",
    "]\n",
    "\n",
    "def split_documents(chunk_size: int, knowledge_base: list):\n",
    "    \"\"\"\n",
    "    Splits documents using character-based logic only (no tokenizer).\n",
    "    Adds overlap, supports Markdown-style separators, prefixes with PDF name, removes duplicates.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        pdf_name = doc.metadata.get(\"pdf_name\") or doc.metadata.get(\"source\", \"Unknown PDF\")\n",
    "        chunks = text_splitter.split_documents([doc])\n",
    "        for chunk in chunks:\n",
    "            chunk.page_content = f\"[{pdf_name[-12:-4]}]\\n{chunk.page_content}\"\n",
    "            docs_processed.append(chunk)\n",
    "\n",
    "    # Remove duplicates by content\n",
    "    seen = set()\n",
    "    return [doc for doc in docs_processed if doc.page_content not in seen and not seen.add(doc.page_content)]\n",
    "\n",
    "\n",
    "def retrieve_and_generate_openai(id :str, query: str) -> str:\n",
    "    docs_processed = split_documents(512, all_documents)\n",
    "    docs = [doc for doc in docs_processed if doc.metadata.get(\"id\") == id]\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"\"\"You are a helpful assistant.\n",
    "    Context:\n",
    "    {context}\n",
    "      Pls extract these information from Context:\n",
    "    - Name: full name of the candidate if not label NA\n",
    "    - Email: valid email address need contain @ in results;if not label NA\n",
    "    - Phone: phone number ;if not label NA\n",
    "    - Skills: a list of technical skills in IT fields and professional/soft skills in IT Fields\n",
    "    - Education: list of degrees with institution name and graduation year  \n",
    "    - Experience: for each job, include:\n",
    "        - Job Title\n",
    "        - Company Name\n",
    "        - Years Worked (start year minus end year)\n",
    "        - Short Description of Responsibilities: extract exact words\n",
    "    - Certifications: list of relevant certifications (if available)  ; if not label NA\n",
    "    - Languages: languages the candidate can speak or write such as English or Spain or Vietnam; if not label NA \n",
    "\n",
    "    COMPARE CONTEXT TO {query}\n",
    "    \n",
    "    Question: Return string similarity metrics score each fields between context and query\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    # Step 3: Call OpenAI\n",
    "    response = model.predict(prompt)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks after splitting: 1930\n",
      "The string similarity metrics score for each field between the context and the query is as follows:\n",
      "\n",
      "- Name: 100% (Both are \"NA\")\n",
      "- Email: 100% (Both are \"NA\")\n",
      "- Phone: 100% (Both are \"NA\")\n",
      "- Skills: 100% (Both lists contain the same skills)\n",
      "- Education: 100% (Both lists contain the same education details)\n",
      "- Experience: 100% (Both lists contain the same job titles, company names, years worked, and responsibilities)\n",
      "- Certifications: 100% (Both lists contain the same certifications)\n",
      "- Languages: 100% (Both are \"NA\") \n",
      "\n",
      "Please note that these scores are based on exact string matches. Any slight variation in wording, punctuation, or order would result in a lower score.\n"
     ]
    }
   ],
   "source": [
    "# Split the loaded documents into chunks (adjust chunk_size as needed)\n",
    "docs_processed = split_documents(512, all_documents)\n",
    "query = results_llm\n",
    "print(retrieve_and_generate_openai(\"33241454\",query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
