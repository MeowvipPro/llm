{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd147ee",
   "metadata": {},
   "source": [
    "#HUY THANH LE SOLUTION - SOURCE DOE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40273f96",
   "metadata": {},
   "source": [
    "# 0. IMPORT LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56899ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dateutil import parser\n",
    "from typing import Optional, List, Tuple\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from transformers import  AutoTokenizer\n",
    "from pypdf import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import kagglehub\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import html\n",
    "import math\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c3a68",
   "metadata": {},
   "source": [
    "# 1. IMPORT LIB & DATA SOUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742de5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"snehaanbhawal/resume-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e1031",
   "metadata": {},
   "source": [
    "# 2. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f6566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV meta data for 120 INFORMATION-TECHNOLOGY resumes.\n",
      "Found 120 PDF files in C:\\Users\\huy.let3\\Desktop\\resume\\data\\data\\INFORMATION-TECHNOLOGY.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = r\"\\resume\\data\\data\\INFORMATION-TECHNOLOGY\"\n",
    "\n",
    "CSV_PATH = r\"\\resume\\Resume\\Resume.csv\"  # Path to the CSV file with resume metadata\n",
    "\n",
    "resume_df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "resume_df = resume_df.loc[resume_df['Category'] == 'INFORMATION-TECHNOLOGY']\n",
    "\n",
    "pdf_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith('.pdf')]\n",
    "\n",
    "print(f\"Loaded CSV meta data for {len(resume_df)} INFORMATION-TECHNOLOGY resumes.\")\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF files in {DATA_DIR}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53503814",
   "metadata": {},
   "source": [
    "# 3. FROM UNSTRUCTURE TO STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "af7e2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_scanned_pdf(pdf_path):\n",
    "    \"\"\"Check if the PDF is scanned by verifying if text can be extracted.\"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            if page.extract_text():\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def extract_text_from_digital_pdf(pdfPath): #digital pdf\n",
    "    text = \"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdfPath)\n",
    "        for page in reader.pages:\n",
    "            pageText = page.extract_text()\n",
    "            if pageText:\n",
    "                text += pageText + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdfPath} using PyPDF2: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_text_from_scanned_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Convert PDF to a list of images (one per page)\n",
    "        images = convert_from_path(pdf_path)\n",
    "        for i, image in enumerate(images):\n",
    "            page_text = pytesseract.image_to_string(image)\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path} with OCR: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(\"\\n\", \" \").replace(\"  \", \" \").strip()\n",
    "\n",
    "\n",
    "def process_pdf(pdf_path):  #Split Flow digital (PRD Reader) or scan (OCR Engine)\n",
    "    if is_scanned_pdf(pdf_path):\n",
    "        print(\"Detected: Scanned PDF (image-based)\")\n",
    "        raw_text = extract_text_from_scanned_pdf(pdf_path)\n",
    "    else:\n",
    "        print(\"Detected: Text-based PDF (digitally generated)\")\n",
    "        raw_text = extract_text_from_digital_pdf(pdf_path)\n",
    "\n",
    "    cleaned = clean_text(raw_text)\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072664bd",
   "metadata": {},
   "source": [
    "# 4. LLM MODEL (OPENAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb24364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "client = OpenAI(api_key = \"YOUR KEY\")\n",
    "\n",
    "def extract_entities_with_gpt(text):\n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "    You are an AI assistant that extracts exact structured candidate information from resumes.\n",
    "\n",
    "    Please extract exactly words the following fields and return the result in valid JSON format:\n",
    "\n",
    "    - Name: full name of the candidate if not label NA\n",
    "    - Email: valid email address need contain @ in results;if not label NA\n",
    "    - Phone: phone number ;if not label NA\n",
    "    - Skills: a list of technical skills in IT fields and professional/soft skills in IT Fields\n",
    "    - Education: list of degrees with institution name and graduation year  \n",
    "    - Experience: for each job, include:\n",
    "        - Job Title\n",
    "        - Company Name\n",
    "        - Years Worked: Each Jobs, calculate by using end year minus start year and Round up with 1 decimal \n",
    "        - Short Description of Responsibilities: extract exact words\n",
    "    - Certifications: list of relevant certifications (if available)  ; if not label NA\n",
    "    - Languages: languages the candidate can speak or write such as English or Spain or Vietnam; if not label NA \n",
    "\n",
    "    Text:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Respond in JSON format.\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4\",\n",
    "        input=[\n",
    "        {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    )\n",
    "    extract = response.output_text\n",
    "    extracted_entities = json.loads(extract)\n",
    "    return extracted_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b0dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: Text-based PDF (digitally generated)\n",
      "Sending to GPT for entity extraction...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Name': 'NA',\n",
       " 'Email': 'NA',\n",
       " 'Phone': 'NA',\n",
       " 'Skills': ['Excellent communication techniques',\n",
       "  'Manufacturing systems integration',\n",
       "  'Multidisciplinary exposure',\n",
       "  'Design instruction creation',\n",
       "  'Project management',\n",
       "  'Complex problem solver',\n",
       "  'Advanced critical thinking',\n",
       "  'SharePoint',\n",
       "  'Microsoft Excel, Project and Visio',\n",
       "  'LAN/WAN protocols',\n",
       "  'Army',\n",
       "  'Cisco',\n",
       "  'counseling',\n",
       "  'customer assistance',\n",
       "  'database',\n",
       "  'documentation',\n",
       "  'Information Technology',\n",
       "  'inventory',\n",
       "  'IP',\n",
       "  'LAN',\n",
       "  'Windows 7',\n",
       "  'Network',\n",
       "  'personnel',\n",
       "  'policies',\n",
       "  'protocols',\n",
       "  'repairs',\n",
       "  'Routing',\n",
       "  'San',\n",
       "  'supervisor',\n",
       "  'test equipment',\n",
       "  'troubleshoot',\n",
       "  'WAN'],\n",
       " 'Education': [{'Degree': 'Certification, Windows 7',\n",
       "   'Institution': 'Microsoft, Fort Bragg, NC',\n",
       "   'Year': '2012'},\n",
       "  {'Degree': 'Certification, Security',\n",
       "   'Institution': 'Comptia, Yong San, Korea',\n",
       "   'Year': '2012'},\n",
       "  {'Degree': 'Distinguished Graduate Certificate, Information Technology (Network Communications) Course',\n",
       "   'Institution': 'U.S. Army',\n",
       "   'Year': '2009'},\n",
       "  {'Degree': 'Certificate, IT Network and Cisco Routing',\n",
       "   'Institution': 'IT Field Services Branch',\n",
       "   'Year': '2009'},\n",
       "  {'Degree': 'Associate of Science : Radiography',\n",
       "   'Institution': 'Northwest Florida State College',\n",
       "   'Year': '2008'},\n",
       "  {'Degree': 'Radiography Certificate',\n",
       "   'Institution': 'IT Tech Prep, Trumbull Career and Technical Center',\n",
       "   'Year': '2001'},\n",
       "  {'Degree': 'Diploma', 'Institution': 'Warren G. Harding', 'Year': '2001'}],\n",
       " 'Experience': [{'Job Title': 'Information Technology Supervisor',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': '3.4',\n",
       "   'Short Description of Responsibilities': 'Supervise up to 10 personnel at one time, delegating tasks, conducting performance evaluations and providing corrective counseling as necessary. Train personnel in the set-up and proper use of IT related equipment while adhering to all policies and procedures. Responsible for the inventory of over $1 million worth of network communications equipment. Tasked by President of the United States to act as supervisor and maintain signal communications for Fort Bragg army base.'},\n",
       "  {'Job Title': 'Information Technology Technician',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': '2.0',\n",
       "   'Short Description of Responsibilities': 'Maintained communications equipment in order to effectively relay confidential and secret information. Utilized electronic test equipment to troubleshoot malfunctioning communications equipment and complete repairs as necessary. Regularly set up and added computer systems to a communication network, installing operation systems, accessing stored programs and utilizing IP addresses. Received training in LAN/WAN protocols.'},\n",
       "  {'Job Title': 'Radiology Technologist',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': '0.4',\n",
       "   'Short Description of Responsibilities': 'Routinely performed radiological examinations in a medical clinic. Competent and experienced in the set-up and adjustment of medical devices or equipment. Regularly provided customer assistance, ensuring all patients received timely and accurate care. Accountable for the accurate documentation via electronic database and file system ensuring all confidentiality was maintained.'},\n",
       "  {'Job Title': 'Command Post Controller',\n",
       "   'Company Name': 'Company Name',\n",
       "   'Years Worked': '4.0',\n",
       "   'Short Description of Responsibilities': 'Provided command, control, communications, and information support throughout operations during peacetime, emergency, and disaster situations. Received and relayed instructions and records, submitting manual and automated data products. Disseminated time-sensitive critical information to senior leaders and support agencies.'}],\n",
       " 'Certifications': ['Army Achievement Medal',\n",
       "  'Air Force Achievement Medal',\n",
       "  'Certificate of Achievement',\n",
       "  'Good Conduct Medal'],\n",
       " 'Languages': 'NA'}"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure_data = process_pdf(r\"data\\data\\INFORMATION-TECHNOLOGY\\33241454.pdf\")\n",
    "results_llm = extract_entities_with_gpt(structure_data)\n",
    "results_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681673c",
   "metadata": {},
   "source": [
    "# 5.Grouth Truth Via with REGEX HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "c7dd51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"NA\"\n",
    "    text = html.unescape(text)\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_resume_entities(html_string):  ### BASED ON SECTION_*\n",
    "    soup = BeautifulSoup(html_string, \"html.parser\")\n",
    "    extracted = {\n",
    "        \"Name\": \"NA\",\n",
    "        \"Email\": \"NA\",\n",
    "        \"Phone\": \"NA\",\n",
    "        \"Skills\": [],\n",
    "        \"Education\": [],\n",
    "        \"Experience\": [],\n",
    "        \"Certifications\": [],\n",
    "        \"Languages\": []\n",
    "    }\n",
    "\n",
    "    # 1. Name\n",
    "    name_section = soup.find(\"div\", id=re.compile(r\"SECTION_NAME1\"))\n",
    "    if name_section:\n",
    "        name_text = clean_text(name_section.get_text(separator=\" \", strip=True))\n",
    "        if name_text:\n",
    "            extracted['Name'] = name_text\n",
    "\n",
    "    # 2. Email\n",
    "    email_match = re.search(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", html_string)\n",
    "    if email_match:\n",
    "        extracted['Email'] = email_match.group()\n",
    "\n",
    "    # 3. Phone\n",
    "    phone_match = re.search(r\"\\b(?:\\+?1[\\s\\-]?)?(?:\\(?\\d{3}\\)?[\\s\\-]?)?\\d{3}[\\s\\-]?\\d{4}\\b\", html_string)\n",
    "    if phone_match:\n",
    "        extracted['Phone'] = phone_match.group()\n",
    "\n",
    "    # 4. Skills\n",
    "    skill_sections = soup.find_all(\"div\", id=re.compile(r\"SECTION_SKLL\"))\n",
    "    for section in skill_sections:\n",
    "        for block in section.find_all(\"div\", class_=\"field\"):\n",
    "            text = clean_text(block.get_text(separator=\" \", strip=True))\n",
    "            if text:\n",
    "                for skill in re.split(r\"[,;•\\-|\\n]+\", text):\n",
    "                    skill_clean = clean_text(skill)\n",
    "                    if skill_clean and skill_clean != \"NA\":\n",
    "                        extracted[\"Skills\"].append(skill_clean)\n",
    "\n",
    "    # 5. Education\n",
    "    education_section = soup.find(\"div\", id=re.compile(\"SECTION_EDUC\"))\n",
    "    if education_section:\n",
    "        entries = education_section.find_all(\"div\", class_=\"singlecolumn\")\n",
    "        for entry in entries:\n",
    "            degree = clean_text(entry.find(\"span\", class_=\"degree\").get_text()) if entry.find(\"span\", class_=\"degree\") else \"NA\"\n",
    "            program = clean_text(entry.find(\"span\", class_=\"programline\").get_text()) if entry.find(\"span\", class_=\"programline\") else \"NA\"\n",
    "            year = clean_text(entry.find(\"span\", class_=\"jobdates\").get_text()) if entry.find(\"span\", class_=\"jobdates\") else \"NA\"\n",
    "            institution = clean_text(entry.find(\"span\", class_=\"companyname\").get_text()) if entry.find(\"span\", class_=\"companyname\") else \"NA\"\n",
    "\n",
    "            edu_item = {\n",
    "                \"Degree\": degree,\n",
    "                \"Program\": program,\n",
    "                \"Year\": year,\n",
    "                \"Institution\": institution\n",
    "            }\n",
    "            extracted[\"Education\"].append(edu_item)\n",
    "\n",
    "    # 6. Experience\n",
    "    experience_section = soup.find(\"div\", id=re.compile(\"SECTION_EXPR\"))\n",
    "    if experience_section:\n",
    "        jobs = experience_section.find_all(\"div\", class_=\"paragraph\")\n",
    "        for job in jobs:\n",
    "            title = clean_text(job.find(\"span\", class_=\"jobtitle\").get_text()) if job.find(\"span\", class_=\"jobtitle\") else \"NA\"\n",
    "            company = clean_text(job.find(\"span\", class_=\"companyname\").get_text()) if job.find(\"span\", class_=\"companyname\") else \"NA\"\n",
    "            description = clean_text(job.find(\"span\", class_=\"jobline\").get_text()) if job.find(\"span\", class_=\"jobline\") else \"NA\"\n",
    "\n",
    "            # Extract 2 dates\n",
    "            years_worked = \"NA\"\n",
    "            dates = job.find_all(\"span\", class_=\"jobdates\", format=True)\n",
    "            if len(dates) == 2:\n",
    "                try:\n",
    "                    start_date = parser.parse(clean_text(dates[0].get_text()), dayfirst=False)\n",
    "                    end_date = parser.parse(clean_text(dates[1].get_text()), dayfirst=False)\n",
    "                    delta_years = (end_date - start_date).days / 365.25\n",
    "                    years_worked = math.ceil(delta_years * 10) / 10\n",
    "                except:\n",
    "                    years_worked = \"NA\"\n",
    "\n",
    "            exp_item = {\n",
    "                \"Job Title\": title,\n",
    "                \"Company Name\": company,\n",
    "                \"Years Worked\": years_worked,\n",
    "                \"Short Description of Responsibilities\": description\n",
    "            }\n",
    "            extracted[\"Experience\"].append(exp_item)\n",
    "\n",
    "    # 7. Certifications\n",
    "    cert_match = re.findall(r\"(?:certified|certification|certificate)[^<\\n]{0,100}\", html_string, flags=re.IGNORECASE)\n",
    "    if cert_match:\n",
    "        certs_clean = [clean_text(c) for c in cert_match]\n",
    "        extracted[\"Certifications\"] = list(set(filter(lambda x: x != \"NA\", certs_clean)))\n",
    "\n",
    "    # 8. Languages\n",
    "    lang_keywords = re.findall(r\"\\b(English|Spanish|Vietnamese|Chinese|French|German|Japanese|Korean)\\b\", html_string, flags=re.IGNORECASE)\n",
    "    if lang_keywords:\n",
    "        extracted[\"Languages\"] = list(set([clean_text(lang) for lang in lang_keywords]))\n",
    "    else:\n",
    "        extracted[\"Languages\"] = \"NA\"\n",
    "\n",
    "    return extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f0740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ✅ Match (Similarity: 1.00)\n",
      "Email: ✅ Match (Similarity: 1.00)\n",
      "Phone: ✅ Match (Similarity: 1.00)\n",
      "Skills: ❌ Mismatch (Similarity: 0.05)\n",
      "Education: ✅ Match (Similarity: 0.87)\n",
      "Experience: ✅ Match (Similarity: 0.84)\n",
      "Certifications: ❌ Mismatch (Similarity: 0.29)\n",
      "Languages: ✅ Match (Similarity: 1.00)\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "def compute_similarity(text1, text2):\n",
    "    embeddings = model.encode([text1, text2], convert_to_tensor=True)\n",
    "    return float(util.cos_sim(embeddings[0], embeddings[1]))\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "\n",
    "ground_truth = extract_resume_entities(resume_df.loc[resume_df['ID'] == 16899268]['Resume_html'].values[0])\n",
    "\n",
    "comparison = {}\n",
    "for key in ground_truth:\n",
    "    response_text = str(results_llm.get(key, \"\")).replace(\"\\n\", \" \").strip().lower()\n",
    "    expected_text = str(ground_truth.get(key, \"\")).replace(\"\\n\", \" \").strip().lower()\n",
    "    similarity = compute_similarity(response_text, expected_text)\n",
    "    comparison[key] = similarity >= SIMILARITY_THRESHOLD\n",
    "    print(f\"{key}: {'✅ Match' if comparison[key] else '❌ Mismatch'} (Similarity: {similarity:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "9c3e208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: Text-based PDF (digitally generated)\n",
      "--------------------------------------------------\n",
      "TESTING ID 27058381\n",
      "Detected: Text-based PDF (digitally generated)\n",
      "--------------------------------------------------\n",
      "TESTING ID 15791766\n",
      "Detected: Text-based PDF (digitally generated)\n",
      "--------------------------------------------------\n",
      "TESTING ID 66832845\n",
      "\n",
      "🔍 Average Similarities per Field:\n",
      "Name              0.715924\n",
      "Email             1.000000\n",
      "Phone             0.723672\n",
      "Skills            0.860838\n",
      "Education         0.722207\n",
      "Experience        0.700774\n",
      "Certifications    0.387761\n",
      "Languages         1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "comparison_records = [] \n",
    "similarity_records = [] \n",
    "\n",
    "sample_files = random.sample(pdf_files, min(3, len(pdf_files)))\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "for x in sample_files:\n",
    "    structure_data = process_pdf(r\"data\\data\\INFORMATION-TECHNOLOGY\" + \"\\\\\" + x)\n",
    "    results_llm = extract_entities_with_gpt(structure_data)  # Uncomment when GPT is used\n",
    "    id_csv = int(x[:-4])    \n",
    "    ground_truth = extract_resume_entities(resume_df.loc[resume_df['ID'] == id_csv]['Resume_html'].values[0])\n",
    "    comparison = {}\n",
    "    similarity_row = {}\n",
    "    print(\"-\"*50)\n",
    "    print(\"TESTING ID\", id_csv)\n",
    "    for key in ground_truth:\n",
    "        response_text = str(results_llm.get(key, \"\")).replace(\"\\n\", \" \").strip().lower()\n",
    "        expected_text = str(ground_truth.get(key, \"\")).replace(\"\\n\", \" \").strip().lower()\n",
    "        similarity = compute_similarity(response_text, expected_text)\n",
    "        comparison[key] = similarity >= SIMILARITY_THRESHOLD\n",
    "        similarity_row[key] = similarity\n",
    "    comparison[\"filename\"] = x\n",
    "    similarity_row[\"filename\"] = x\n",
    "    comparison_records.append(comparison)\n",
    "    similarity_records.append(similarity_row)\n",
    "comparison_df = pd.DataFrame(comparison_records)\n",
    "similarity_df = pd.DataFrame(similarity_records)\n",
    "avg_similarities = similarity_df.drop(columns=['filename']).mean()\n",
    "print(\"\\n🔍 Average Similarities per Field:\")\n",
    "print(avg_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e56ac",
   "metadata": {},
   "source": [
    "# 6. SELF-EVALUATION LLM (Context vs Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f42786",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4\",  # or \"gpt-3.5-turbo\"\n",
    "    temperature=0.5,\n",
    "    openai_api_key= 'YOUR KEY' )\n",
    "\n",
    "DATA_DIR = r\"\\resume\\data\\data\\INFORMATION-TECHNOLOGY\"\n",
    "\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \", \"```\\n\", \"\\n\\\\*\\\\*\\\\*+\\n\", \"\\n---+\\n\", \"\\n___+\\n\", \"\\n\\n\", \"\\n\", \" \", \"\"\n",
    "]\n",
    "\n",
    "def split_documents(chunk_size: int, knowledge_base: list):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        pdf_name = doc.metadata.get(\"pdf_name\") or doc.metadata.get(\"source\", \"Unknown PDF\")\n",
    "        chunks = text_splitter.split_documents([doc])\n",
    "        for chunk in chunks:\n",
    "            chunk.page_content = f\"[{pdf_name[-12:-4]}]\\n{chunk.page_content}\"\n",
    "            docs_processed.append(chunk)\n",
    "\n",
    "    # Remove duplicates by content\n",
    "    seen = set()\n",
    "    return [doc for doc in docs_processed if doc.page_content not in seen and not seen.add(doc.page_content)]\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(DATA_DIR, filename))\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"pdf_name\"] = filename\n",
    "            doc.metadata[\"id\"] = filename[:-4]  # e.g., 123.pdf -> id = '123'\n",
    "            all_documents.append(doc)\n",
    "\n",
    "docs_processed = split_documents(512, all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "0e22a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_generate_openai(id: str, query: str, docs_processed) -> str:\n",
    "    docs = [doc for doc in docs_processed if doc.metadata.get(\"id\") == id]\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"\"\"You are a helpful assistant.\n",
    "    Context: {context}\n",
    "\n",
    "    Pls extract these information from Context HTML:\n",
    "    - Name: full name of the candidate; if not, label NA\n",
    "    - Email: valid email address (must contain @); if not, label NA\n",
    "    - Phone: phone number; if not, label NA\n",
    "    - Skills: list of technical and soft skills relevant to IT\n",
    "    - Education: list of degrees with institution name and graduation year  \n",
    "    - Experience: for each job, include:\n",
    "        - Job Title\n",
    "        - Company Name\n",
    "        - Years Worked: end year - start year, round up with 1 decimal\n",
    "        - Short Description of Responsibilities (exact wording)\n",
    "    \n",
    "    - Certifications: list of relevant certifications; if none, label NA\n",
    "\n",
    "    - Languages: list of languages spoken or written (e.g., English, Spanish, Vietnamese); if none, label NA\n",
    "\n",
    "    Pls return only similarity score float from 0 to 1 each fields between query: {query}, and context as json files \n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = model.predict(prompt)\n",
    "    return response.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "6dd5bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: Text-based PDF (digitally generated)\n",
      "--------------------------------------------------\n",
      "TESTING ID 66832845\n",
      "{'Name': 0.0, 'Email': 0.0, 'Phone': 0.0, 'Skills': 1.0, 'Education': 1.0, 'Experience': 1.0, 'Certifications': 1.0, 'Languages': 1.0}\n",
      "Detected: Text-based PDF (digitally generated)\n",
      "--------------------------------------------------\n",
      "TESTING ID 66832845\n",
      "{'Name': 0.0, 'Email': 0.0, 'Phone': 0.0, 'Skills': 1.0, 'Education': 1.0, 'Experience': 1.0, 'Certifications': 1.0, 'Languages': 1.0}\n",
      "Detected: Text-based PDF (digitally generated)\n",
      "--------------------------------------------------\n",
      "TESTING ID 66832845\n",
      "{'Name': 0.0, 'Email': 0.0, 'Phone': 0.0, 'Skills': 1.0, 'Education': 1.0, 'Experience': 0.7, 'Certifications': 0.0, 'Languages': 0.0}\n"
     ]
    }
   ],
   "source": [
    "for x in sample_files:\n",
    "    structure_data = process_pdf(r\"data\\data\\INFORMATION-TECHNOLOGY\" + \"\\\\\" + x)\n",
    "    results_llm = extract_entities_with_gpt(structure_data)  # Uncomment when GPT is used\n",
    "    print(\"-\"*50)\n",
    "    print(\"TESTING ID\", id_csv)\n",
    "    output = retrieve_and_generate_openai(x[:-4],results_llm,docs_processed)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9446f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
